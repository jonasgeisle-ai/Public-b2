{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a4ab10b",
   "metadata": {},
   "source": [
    " Dataset A (S&P500 + headlines)\n",
    "\n",
    "We build a next-day direction label from S&P 500 close prices and compares two simple text-based approaches\n",
    "\n",
    " BOW baseline hashed bag-of-words from daily headlines then logistic regression  \n",
    "\n",
    "FinBERT features daily mean sentiment probabilities and headline count then logistic regression\n",
    "\n",
    "Leakage note: \n",
    "I use a time split (train ≤ 2018, val 2019–2021, test ≥ 2022).\n",
    " If any AI were used for, I will cite them in the written report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc3ab56",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b20651de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cf4e3d",
   "metadata": {},
   "source": [
    "Tokenization + stable hashing for BOW\n",
    "\n",
    "Baseline tokenizer: keep only [A-Za-z] tokens\n",
    "\n",
    "For hashing, I avoid Python's built-in hash() because it changes between runs, which took longer to realize than i am proud of. \n",
    "Instead I use md5 and map tokens into a fixed number of bins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec675225",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_re = re.compile(r\"[a-zA-Z]+\")\n",
    "\n",
    "def toks(s):\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    return word_re.findall(s.lower())\n",
    "\n",
    "def stable_hash(word: str, bins: int) -> int:\n",
    "    # stable across runs.... unlike python hash\n",
    "    h = hashlib.md5(word.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16) % bins\n",
    "\n",
    "def make_hash_X(texts, bins=5000):\n",
    "    # hashed bag-of-words with log1p counts\n",
    "    X = np.zeros((len(texts), bins), dtype=np.float32)\n",
    "    for i, t in enumerate(texts):\n",
    "        for w in toks(t):\n",
    "            X[i, stable_hash(w, bins)] += 1.0\n",
    "    X = np.log1p(X)\n",
    "    return torch.tensor(X, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21259d",
   "metadata": {},
   "source": [
    "Logistic regression in PyTorch + evaluation setups\n",
    "\n",
    "Model: one Linear layer with BCEWithLogitsLoss binary classification\n",
    "\n",
    "I print validation accuracy each epoch just to make sure it works as it should.\n",
    "I ahd problems in the beginings making sure it would do it right. \n",
    "\n",
    "\n",
    "print_balance shows the label balance (up-rate) in each split\n",
    "majority_baseline_acc predicts the most common label in train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb5b6feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lr(Xtr, ytr, Xva, yva, epochs=15, lr=0.2):\n",
    "    dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    Xtr, ytr = Xtr.to(dev), ytr.to(dev)\n",
    "    Xva, yva = Xva.to(dev), yva.to(dev)\n",
    "\n",
    "    m = torch.nn.Linear(Xtr.shape[1], 1).to(dev)\n",
    "    opt = torch.optim.SGD(m.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "#trainning loop\n",
    "    for ep in range(1, epochs + 1):\n",
    "        m.train()\n",
    "        opt.zero_grad()\n",
    "        logits = m(Xtr).squeeze(1)\n",
    "        loss = loss_fn(logits, ytr)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        m.eval()\n",
    "        with torch.no_grad():\n",
    "            p = torch.sigmoid(m(Xva).squeeze(1))\n",
    "            pred = (p >= 0.5).float()\n",
    "            acc = (pred == yva).float().mean().item()\n",
    "\n",
    "        print(f\"ep {ep:02d} | loss {loss.item():.4f} | val_acc {acc:.4f}\")\n",
    "\n",
    "    return m\n",
    "\n",
    "def acc(m, X, y, name=\"test\"):\n",
    "    dev = next(m.parameters()).device\n",
    "    X, y = X.to(dev), y.to(dev)\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        p = torch.sigmoid(m(X).squeeze(1))\n",
    "        pred = (p >= 0.5).float()\n",
    "        a = (pred == y).float().mean().item()\n",
    "    print(f\"{name}_acc {a:.4f}\")\n",
    "    return a\n",
    "\n",
    "def print_balance(y, name):\n",
    "    y = np.asarray(y).astype(float)\n",
    "    if len(y) == 0:\n",
    "        print(f\"{name}: n=0\")\n",
    "        return\n",
    "    print(f\"{name}: n={len(y)} | up_rate={y.mean():.3f} | down_rate={1-y.mean():.3f}\")\n",
    "\n",
    "def majority_baseline_acc(y_train, y_test, name=\"baseline\"):\n",
    "    # predict the major class from train\n",
    "    p = 1.0 if np.mean(y_train) >= 0.5 else 0.0\n",
    "    pred = np.full_like(y_test, p, dtype=float)\n",
    "    a = (pred == y_test).mean()\n",
    "    print(f\"{name}_acc {a:.4f} (predict={int(p)})\")\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7373f",
   "metadata": {},
   "source": [
    "FinBERT daily features + caching\n",
    "\n",
    "For each date:\n",
    "runs finbert on each headline\n",
    "average the predicted probabilities across headlines\n",
    "features per day: p_neg, p_neu, p_pos\n",
    "plus n_headlines as a simple intensity feature\n",
    "\n",
    "OBS AutoTokenizer fir finbgert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b154e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finbert_feats(day_to_titles, model_name=\"ProsusAI/finbert\", bs=16, max_len=64):\n",
    "    dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokz = AutoTokenizer.from_pretrained(model_name)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(model_name).to(dev)\n",
    "    mdl.eval()\n",
    "\n",
    "    dates = []\n",
    "    feats = []\n",
    "    n_news = []\n",
    "\n",
    "    for d, titles in day_to_titles.items():\n",
    "        titles = [t for t in titles if isinstance(t, str) and t.strip() != \"\"]\n",
    "        n_news.append(len(titles))\n",
    "\n",
    "        if len(titles) == 0:\n",
    "            dates.append(d)\n",
    "            feats.append([0.0, 1.0, 0.0])  # default neutral\n",
    "            continue\n",
    "\n",
    "        probs_all = []\n",
    "        for i in range(0, len(titles), bs):\n",
    "            batch = titles[i:i + bs]\n",
    "            enc = tokz(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            enc = {k: v.to(dev) for k, v in enc.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = mdl(**enc)\n",
    "                probs = torch.softmax(out.logits, dim=1)  # neg/neu/pos\n",
    "                probs_all.append(probs.detach().cpu())\n",
    "\n",
    "        probs_all = torch.cat(probs_all, dim=0)\n",
    "        mean_probs = probs_all.mean(dim=0).tolist()\n",
    "\n",
    "        dates.append(d)\n",
    "        feats.append(mean_probs)\n",
    "\n",
    "    f = pd.DataFrame(feats, columns=[\"p_neg\", \"p_neu\", \"p_pos\"])\n",
    "    f[\"date\"] = dates\n",
    "    f[\"n_headlines\"] = n_news\n",
    "    return f\n",
    "\n",
    "def get_finbert_daily(day_df, cache_path=\"finbert_daily_cache.csv\"):\n",
    "    # expects columns: date, title_list\n",
    "    if os.path.exists(cache_path):\n",
    "        f = pd.read_csv(cache_path)\n",
    "        f[\"date\"] = pd.to_datetime(f[\"date\"]).dt.date\n",
    "        return f\n",
    "\n",
    "    day_map = dict(zip(day_df[\"date\"], day_df[\"title_list\"]))\n",
    "    f = finbert_feats(day_map)\n",
    "    f.to_csv(cache_path, index=False)\n",
    "    return f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe14228",
   "metadata": {},
   "source": [
    "Load data + build daily dataset\n",
    "\n",
    "OBS: Change bow or finbert in below mode!!!\n",
    "\n",
    "Columns in the CSV:\n",
    "- Title = headline\n",
    "- Date = headline date\n",
    "- CP = close price for the day\n",
    "\n",
    "Daily aggregation:\n",
    "text: headlines joined for BOW\n",
    "title_list: list of headlines for FinBERT\n",
    "\n",
    "Label:\n",
    "y = 1 if next day's close is higher than today's close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9a36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 2222 747 537\n",
      "date ranges: 2008-01-02 → 2018-12-31 | 2019-01-02 → 2021-12-31 | 2022-01-03 → 2024-03-01\n",
      "train: n=2222 | up_rate=0.543 | down_rate=0.457\n",
      "val: n=747 | up_rate=0.577 | down_rate=0.423\n",
      "test: n=537 | up_rate=0.495 | down_rate=0.505\n",
      "majority_baseline_acc 0.4953 (predict=1)\n",
      "         date    close  close_next    y\n",
      "0  2008-01-02  1447.16     1447.16  0.0\n",
      "1  2008-01-03  1447.16     1416.18  0.0\n",
      "2  2008-01-07  1416.18     1409.13  0.0\n"
     ]
    }
   ],
   "source": [
    "CSV_PATH = \"sp500_headlines_2008_2024.csv\" \n",
    "MODE = \"finbert\"  # \"bow\" or finbert\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "df[\"date\"]  = pd.to_datetime(df[\"Date\"], errors=\"coerce\").dt.date\n",
    "df[\"close\"] = pd.to_numeric(df[\"CP\"], errors=\"coerce\")\n",
    "df[\"title\"] = df[\"Title\"].astype(str)\n",
    "\n",
    "df = df.dropna(subset=[\"date\", \"close\", \"title\"]).copy()\n",
    "\n",
    "g = df.groupby(\"date\")\n",
    "day_text  = g[\"title\"].apply(lambda s: \" . \".join(s.tolist())).reset_index(name=\"text\")\n",
    "day_list  = g[\"title\"].apply(lambda s: s.tolist()).reset_index(name=\"title_list\")\n",
    "day_close = g[\"close\"].last().reset_index(name=\"close\")\n",
    "\n",
    "day = day_text.merge(day_close, on=\"date\").merge(day_list, on=\"date\")\n",
    "day = day.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# label: next day up/down\n",
    "day[\"close_next\"] = day[\"close\"].shift(-1)\n",
    "day = day.dropna(subset=[\"close_next\"]).copy()\n",
    "day[\"y\"] = (day[\"close_next\"] > day[\"close\"]).astype(np.float32)\n",
    "\n",
    "day[\"year\"] = pd.Series(day[\"date\"]).apply(lambda d: d.year)\n",
    "\n",
    "train = day[day[\"year\"] <= 2018].copy()\n",
    "val   = day[(day[\"year\"] >= 2019) & (day[\"year\"] <= 2021)].copy()\n",
    "test  = day[day[\"year\"] >= 2022].copy()\n",
    "\n",
    "print(\"rows:\", len(train), len(val), len(test))\n",
    "print(\"date ranges:\",\n",
    "      min(train[\"date\"]), \"→\", max(train[\"date\"]),\n",
    "      \"|\", min(val[\"date\"]), \"→\", max(val[\"date\"]),\n",
    "      \"|\", min(test[\"date\"]), \"→\", max(test[\"date\"]))\n",
    "\n",
    "print_balance(train[\"y\"].values, \"train\")\n",
    "print_balance(val[\"y\"].values, \"val\")\n",
    "print_balance(test[\"y\"].values, \"test\")\n",
    "\n",
    "majority_baseline_acc(train[\"y\"].values, test[\"y\"].values, name=\"majority_baseline\")\n",
    "\n",
    "# tensors\n",
    "ytr = torch.tensor(train[\"y\"].values, dtype=torch.float32)\n",
    "yva = torch.tensor(val[\"y\"].values, dtype=torch.float32)\n",
    "yte = torch.tensor(test[\"y\"].values, dtype=torch.float32)\n",
    "\n",
    "# quick check for safty\n",
    "print(day[[\"date\", \"close\", \"close_next\", \"y\"]].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c7e71",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If MODE == \"bow\": hashed bag-of-words (5000 bins) + logistic regression  \n",
    "If MODE == \"finbert\": daily FinBERT sentiment probs (+ headline count) + logistic regression\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a10be89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 01 | loss 0.6928 | val_acc 0.5502\n",
      "ep 02 | loss 0.6920 | val_acc 0.5622\n",
      "ep 03 | loss 0.6914 | val_acc 0.5676\n",
      "ep 04 | loss 0.6908 | val_acc 0.5783\n",
      "ep 05 | loss 0.6903 | val_acc 0.5770\n",
      "ep 06 | loss 0.6899 | val_acc 0.5770\n",
      "ep 07 | loss 0.6895 | val_acc 0.5770\n",
      "ep 08 | loss 0.6891 | val_acc 0.5770\n",
      "ep 09 | loss 0.6888 | val_acc 0.5770\n",
      "ep 10 | loss 0.6884 | val_acc 0.5770\n",
      "ep 11 | loss 0.6881 | val_acc 0.5770\n",
      "ep 12 | loss 0.6878 | val_acc 0.5770\n",
      "ep 13 | loss 0.6875 | val_acc 0.5770\n",
      "ep 14 | loss 0.6872 | val_acc 0.5770\n",
      "ep 15 | loss 0.6869 | val_acc 0.5770\n",
      "test_acc 0.4953\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"bow\":\n",
    "    Xtr = make_hash_X(train[\"text\"].tolist(), bins=5000)\n",
    "    Xva = make_hash_X(val[\"text\"].tolist(), bins=5000)\n",
    "    Xte = make_hash_X(test[\"text\"].tolist(), bins=5000)\n",
    "\n",
    "    m = train_lr(Xtr, ytr, Xva, yva, epochs=15, lr=0.2)\n",
    "    acc(m, Xte, yte, name=\"test\")\n",
    "\n",
    "elif MODE == \"finbert\":\n",
    "    all_f = get_finbert_daily(day[[\"date\", \"title_list\"]].copy(), cache_path=\"finbert_daily_cache.csv\")\n",
    "\n",
    "    ftr = train[[\"date\"]].merge(all_f, on=\"date\", how=\"left\")\n",
    "    fva = val[[\"date\"]].merge(all_f, on=\"date\", how=\"left\")\n",
    "    fte = test[[\"date\"]].merge(all_f, on=\"date\", how=\"left\")\n",
    "\n",
    "    Xtr = torch.tensor(ftr[[\"p_neg\", \"p_neu\", \"p_pos\", \"n_headlines\"]].values, dtype=torch.float32)\n",
    "    Xva = torch.tensor(fva[[\"p_neg\", \"p_neu\", \"p_pos\", \"n_headlines\"]].values, dtype=torch.float32)\n",
    "    Xte = torch.tensor(fte[[\"p_neg\", \"p_neu\", \"p_pos\", \"n_headlines\"]].values, dtype=torch.float32)\n",
    "\n",
    "    m = train_lr(Xtr, ytr, Xva, yva, epochs=30, lr=0.5)\n",
    "    acc(m, Xte, yte, name=\"test\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"mistake were made\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3d1ed4",
   "metadata": {},
   "source": [
    "\n",
    "Note to self\n",
    "\n",
    "Why time split matter\n",
    "\n",
    "Why daily aggregation is a simpli\n",
    "\n",
    "Why accuracyclose to 50%   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
